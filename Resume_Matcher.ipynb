{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69bc3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2,docx\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397d85e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mukun\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mukun\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08350dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resume_text(file_name):\n",
    "    text = \"\"\n",
    "    if re.search('.pdf',file_name):\n",
    "        with open(file_name, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "\n",
    "    if re.search('.doc',file_name):\n",
    "        doc = docx.Document(file_name)\n",
    "        for element in doc.element.body:\n",
    "            if element.tag.endswith('p'):\n",
    "                paragraph = docx.text.paragraph.Paragraph(element, doc)\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            elif element.tag.endswith('tbl'):\n",
    "                table = docx.table.Table(element, doc)\n",
    "                for row in table.rows:\n",
    "                    for cell in row.cells:\n",
    "                        cell_text = cell.text.strip() if cell.text else ''\n",
    "                        text += cell_text + \"\\t\"\n",
    "                    text += \"\\n\"\n",
    "    return text      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8212fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_newlines_and_spaces(text):\n",
    "    # Replace all newline characters with spaces and normalize multiple spaces\n",
    "    return ' '.join(text.replace('\\n', ' ').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00fe5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd = '''Title : Azure  Data Engineer\n",
    "Experience: 4-6 Years \n",
    "Location: Remote\n",
    "Budget : Below 25 LPA\n",
    "•\tExtensive hands-on experience in ADF, Data Bricks\n",
    "•\tFamiliarity with ETL tools and techniques\n",
    "•\tHands on experience with  Microsoft Azure,\n",
    "•\tGood Communication skills\n",
    "•\tShould be able to work independently and own the client deliverables\n",
    "•\tPrior experience with Azure cloud resources\n",
    "•\tFamiliar with programming with GitHub, CICD, Docker.\n",
    "'''.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944cf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'VNC_Vignesh_QA_Professional.docx'\n",
    "resume_text = normalize_newlines_and_spaces(extract_resume_text(file_name).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b936c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_text = '''Title : Azure  Data Engineer\n",
    "Experience: 4-6 Years \n",
    "Location: Remote\n",
    "Budget : Below 25 LPA\n",
    "•\tExtensive hands-on experience in ADF, Data Bricks\n",
    "•\tFamiliarity with ETL tools and techniques\n",
    "•\tHands on experience with  Microsoft Azure,\n",
    "•\tGood Communication skills\n",
    "•\tShould be able to work independently and own the client deliverables\n",
    "•\tPrior experience with Azure cloud resources\n",
    "•\tFamiliar with programming with GitHub, CICD, Docker.\n",
    "'''.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88530c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    special_chars = ['■', '•', '●', '-', '*', '➢','.','/']\n",
    "    # Replace each special character with a space around it\n",
    "    for char in special_chars:\n",
    "        text = text.replace(char, f' {char} ')\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d86274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_text = preprocess_text(jd_text)\n",
    "resume_text = preprocess_text(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5488ed8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "jd_tokens = word_tokenize(jd_text)\n",
    "resume_tokens = word_tokenize(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37b683f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words from a list of words\n",
    "def remove_stop_words(word_list):\n",
    "    return [word for word in word_list if word not in stop_words]\n",
    "\n",
    "pre_final_jd = remove_stop_words(jd_tokens)\n",
    "pre_final_resume = remove_stop_words(resume_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ff89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords and punctuation\n",
    "filtered_jd = [word for word in pre_final_jd if re.match(r'[a-z0-9]', word)]\n",
    "filtered_resume = [word for word in pre_final_resume if re.match(r'[a-z0-9]', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2179ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "# Function to get POS tag for accurate lemmatization\n",
    "def get_pos(word):\n",
    "    pos_tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": \"a\", \"N\": \"n\", \"V\": \"v\", \"R\": \"r\"}\n",
    "    return tag_dict.get(pos_tag, \"n\")\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_jd = [lemmatizer.lemmatize(word, get_pos(word)) for word in filtered_jd]\n",
    "lemmatized_resume = [lemmatizer.lemmatize(word, get_pos(word)) for word in filtered_resume]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "046421fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_jd = ' '.join(lemmatized_jd)\n",
    "final_resume = ' '.join(lemmatized_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b169124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the job description and resume\n",
    "job_description_embedding = model.encode(final_jd)\n",
    "resume_embedding = model.encode(final_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "860cafbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.36\n"
     ]
    }
   ],
   "source": [
    "similarity_score = cosine_similarity([resume_embedding], [job_description_embedding])[0][0]\n",
    "\n",
    "# Print similarity score\n",
    "print(f\"Similarity Score: {similarity_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
